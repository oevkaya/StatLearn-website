# About the application of CART in R


# Note that the R implementation of the CART algorithm is called RPART
# (Recursive Partitioning And Regression Trees) available in a package of the same name.

# Loading required R packages ---------------------------------------------

# tidyverse for easy data manipulation and visualization
# caret for easy machine learning workflow
# rpart for computing decision tree models

library(tidyverse)
library(caret)
library(rpart)


# Decision Tree Algorithm -------------------------------------------------

# The algorithm of decision tree models works by repeatedly partitioning the data
# into multiple sub-spaces, so that the outcomes in each final sub-space is as homogeneous as possible.
# This approach is technically called recursive partitioning.

# The decision rules generated by the CART predictive model are generally visualized as a binary tree.
data(iris)
head(iris)

model <- rpart(Species ~ ., data = iris)
# This is a large object including various details, please read help file for
# rpart, by calling ?rpart
class(model)

# Some details
str(model)

# ie. the complexity parameter at which this split will collapse
model$frame$complexity

par(xpd = NA) # otherwise on some devices the text is clipped
plot(model)
text(model, digits = 3)

# The different rules in tree can be printed as follow:
print(model, digits = 2)

# Some Notes
# The resulting tree is composed of decision nodes, branches and leaf nodes.
# The tree is placed from upside to down, so the root is at the top and leaves
# indicating the outcome is put at the bottom

# The tree grows from the top (root), at each node the algorithm decides
# the best split cutoff that results to the greatest purity (or homogeneity) in each subpartition

# The tree will stop growing by the following three criteria (Zhang 2016):
  # 1. all leaf nodes are pure with a single class;
  # 2. a pre-specified minimum number of training observations that cannot be assigned to each leaf nodes with any splitting methods;
  # 3. The number of observations in the leaf node reaches the pre-specified minimum one.

# A fully grown tree will overfit the training data and the resulting model might not be performant
# for predicting the outcome of new test data. Techniques, such as pruning, are used to control this problem.

# Making prediction for the fitted model
newdata <- data.frame(
  Sepal.Length = 6.5, Sepal.Width = 3.0,
  Petal.Length = 5.2, Petal.Width = 2.0
)

# play with these a bit !
model %>% predict(newdata, "class")


# Choosing the trees split points -----------------------------------------

# - Technically, for regression modeling, the split cut-off is defined so that
# the residual sum of squared error (RSS) is minimized across the training samples
# that fall within the subpartition

# - In classification settings, the split point is defined so that the population in subpartitions
# are pure as much as possible. Two measures of purity are generally used,
# including the Gini index and the entropy (or information gain).

# The sum is computed across the different categories or classes in the outcome variable.
# The Gini index and the entropy varie from 0 (greatest purity) to 1 (maximum degree of impurity)


# Classification trees ----------------------------------------------------

# Data set: PimaIndiansDiabetes2 [in mlbench package]
library(mlbench)

# Load the data and remove NAs
data("PimaIndiansDiabetes2", package = "mlbench")
head(PimaIndiansDiabetes2)

# Help for the data set, for the details of the variables
?PimaIndiansDiabetes2

# Some details for a variable
class(PimaIndiansDiabetes2$pregnant)

# Number of missing values
sum(is.na(PimaIndiansDiabetes2))

# Simply, we removed those missing values
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)

# Inspect the data
head(PimaIndiansDiabetes2)

# Split the data into training and test set, using caret package
set.seed(123)
training.samples <- PimaIndiansDiabetes2$diabetes %>%
  createDataPartition(p = 0.8, list = FALSE)

head(training.samples)

# Splitting the whole data frame with respect to above rule: 80%- 20%
train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]


# Fully grown trees -------------------------------------------------------

# Build the model
set.seed(442)
model1 <- rpart(diabetes ~ ., data = train.data, method = "class")

class(model1)

# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)

# About variable importance
model1$variable.importance


# Make predictions on the test data
predicted.classes <- model1 %>%
  predict(test.data, type = "class")
head(predicted.classes)

# Compute model accuracy rate on test data
mean(predicted.classes == test.data$diabetes)

# Note that The overall accuracy of our tree model is 78%, which is not so bad.


# Pruning the tree --------------------------------------------------------

# Briefly, our goal here is to see if a smaller subtree can give us comparable results to the
# fully grown tree. If yes, we should go for the simpler tree because it reduces
# the likelihood of overfitting.

# In rpart package, this is controlled by the complexity parameter (cp), which imposes a penalty to the
# tree for having two many splits. The default value is 0.01. The higher the cp, the smaller the tree.

# Pruning can be easily performed in the caret package workflow, which invokes the rpart method
# for automatically testing different possible values of cp,
# then choose the optimal cp that maximize the cross-validation accuracy,
# and fit the final best CART model that explains the best our data.

# Fit the model on the training set
set.seed(442)

# Try also 10-fold CV below !
model2 <- train(
  diabetes ~., data = train.data, method = "rpart",
  trControl = trainControl("cv", number = 5),
  tuneLength = 10
)

# Some details for the train function from caret package
# ?train

summary(model2)
# Plot model accuracy vs different values of
# cp (complexity parameter)
plot(model2)

# Print the best tuning parameter cp that
# maximizes the model accuracy
model2$bestTune

# Plot the final tree model
par(xpd = NA) # Avoid clipping the text in some device
plot(model2$finalModel)
text(model2$finalModel,  digits = 3)

# Decision rules in the model, as a prunned tree
model2$finalModel

# Make predictions on the test data
predicted.classes <- model2 %>% predict(test.data)
# Compute model accuracy rate on test data
mean(predicted.classes == test.data$diabetes)


# Regression trees --------------------------------------------------------

# Similarly to classification trees,
# the following R code uses the caret package to build regression trees and
# to predict the output of a new test data set.

# Data set: We will use the Boston data set [in MASS package]
library(MASS)
data(Boston)

# OR
# Load the data as
# data("Boston", package = "MASS")

# Inspect the data
# head(Boston)

# From our book
# Similar to Validation here ! by manually
set.seed(442)
train <- sample(1: nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~., Boston , subset = train)
summary(tree.boston)

plot(tree.boston)
text(tree.boston, pretty = 0)
# It is worth noting that we could have fit a much bigger tree, by passing
# control = tree.control(nobs = length(train), mindev = 0) into the
# tree() function

# CV on tree.boston
cv.boston <- cv.tree(tree.boston)
# Investigate cv.boston ?

plot(cv.boston$size, cv.boston$dev, type = "b")

# About prunning trees
prune.boston <- prune.tree(tree.boston, best = 3) # Play with best value !
plot(prune.boston)
text(prune.boston , pretty = 0)

yhat <- predict(tree.boston , newdata = Boston[-train , ])
boston.test <- Boston[-train, "medv"]

plot(yhat, boston.test)
abline(0, 1)
mean(( yhat - boston.test)^2)

# OR do it automaticall via caret package help
# Split the data into training and test set with caret package
set.seed(442)
training.samples <- Boston$medv %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]

# Create the regression tree ----------------------------------------------
# Here, the best cp value is the one that minimize the prediction error RMSE
# The lower the RMSE, the better the model.

# Fit the model on the training set
set.seed(442)
model <- train(
  medv ~., data = train.data, method = "rpart",
  trControl = trainControl("cv", number = 5),
  tuneLength = 10
)
# Plot model error vs different values of
# cp (complexity parameter)
plot(model)
# Print the best tuning parameter cp that
# minimize the model RMSE
model$bestTune

# Some other performance measures
model$results

# Plot the final tree model
par(xpd = NA) # Avoid clipping the text in some device
plot(model$finalModel)
text(model$finalModel, digits = 3)

# Decision rules in the model
model$finalModel
# Make predictions on the test data
predictions <- model %>% predict(test.data)
head(predictions)
# Compute the prediction error RMSE
RMSE(predictions, test.data$medv)


# Beyond CART -------------------------------------------------------------

# Here we apply bagging and random forests to the Boston data, using the
# randomForest package in R. The exact results obtained in this section may
# depend on the version of R and the version of the randomForest package
# installed on your computer

# install.packages("randomForest")
library(randomForest)

set.seed(442)
bag.boston <- randomForest(medv ~., data = train.data,
                           mtry = 12, importance = TRUE)

# Check details for the function above
# ?randomForest

bag.boston
summary(bag.boston)

# Error versus tree numbers
plot(bag.boston)

# The argument mtry = 12 indicates that all 12 predictors should be considered
# for each split of the treeâ€”in other words, that bagging should be done.
# How well does this bagged model perform on the test set?

yhat.bag <- predict(bag.boston, newdata = test.data)

plot(yhat.bag, test.data$medv)
abline(0, 1)
mean((yhat.bag - test.data$medv)^2)

# We could change the number of trees grown by randomForest() using the ntree argument

bag.boston <- randomForest(medv ~., data = train.data, mtry = 12, ntree = 25)

yhat.bag <- predict(bag.boston, newdata = test.data)
mean((yhat.bag - test.data$medv)^2)

# Growing a random forest proceeds in exactly the same way, except that
# we use a smaller value of the mtry argument !
set.seed(442)
rf.boston <- randomForest(medv ~ ., data = Boston ,
                    subset = train , mtry = 6, importance = TRUE)

yhat.rf <- predict(rf.boston, newdata = Boston[-train , ])
mean(( yhat.rf - boston.test)^2)

# Using the importance() function, we can view the importance of each
# variable
# larger the IncNodePurity, the more important the variables
importance(rf.boston)

# Plots of these importance measures can be produced
# using the varImpPlot() function
varImpPlot(rf.boston)

# TRY TO RUN OTHER ENSEMBLE MODELS FOR THE SAME BOSTON DATA !

# Reference  --------------------------------------------------------------

# http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/141-cart-model-decision-tree-essentials/

# Another example in

# https://rpubs.com/minma/cart_with_rpart
